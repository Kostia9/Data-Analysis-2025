{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOEzXoyuuq4zHRP4J4/GvwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kostia9/Data-Analysis-2025/blob/main/Lab4/Lab4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notebook-title"
      },
      "source": [
        "# Генерація тексту за допомогою моделі-трансформера (GPT-подібна архітектура)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import math, numpy as np\n",
        "import re, string\n",
        "from tqdm import tqdm\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Гіперпараметри (налаштування) ---\n",
        "batch_size = 64        # скільки незалежних послідовностей обробляємо паралельно\n",
        "block_size = 256       # максимальна довжина контексту для передбачення\n",
        "max_iters = 20000      # кількість кроків навчання\n",
        "eval_interval = 1000   # як часто оцінювати втрати (loss)\n",
        "learning_rate = 6e-4   # швидкість навчання\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # використовуємо GPU, якщо є\n",
        "eval_iters = 100       # скільки батчів використовувати для оцінки втрат\n",
        "n_embd = 768           # розмірність векторів (embedding dimension)\n",
        "n_head = 12            # кількість голів (heads) у self-attention\n",
        "n_layer = 12           # кількість блоків трансформера\n",
        "dropout = 0            # регуляризація (for pretraining 0 is good)\n",
        "vocab_size = 32768     # розмір словника: кількість можливих токенів\n",
        "# -----------------------------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsjNnDqe6R1G",
        "outputId": "eceda226-856d-442d-e25b-919a4201631f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bbda0af5f90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset\n",
        "Dataset contains fiction (novels, prose, and some poetry) scraped from two public libraries;"
      ],
      "metadata": {
        "id": "Sl1kpD53o2qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://lang.org.ua/static/downloads/ubertext2.0/fiction/cleansed/ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2\n",
        "!bunzip2 -f ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2\n",
        "\n",
        "CORPUS_FILE = \"ubertext.fiction.filter_rus_gcld+short.text_only.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf0MjUToxsC0",
        "outputId": "b9bb87ef-695f-4ccc-f48c-567fb562ceda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 14:16:14--  https://lang.org.ua/static/downloads/ubertext2.0/fiction/cleansed/ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2\n",
            "Resolving lang.org.ua (lang.org.ua)... 65.21.91.242\n",
            "Connecting to lang.org.ua (lang.org.ua)|65.21.91.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433084760 (413M) [application/octet-stream]\n",
            "Saving to: ‘ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2’\n",
            "\n",
            "ubertext.fiction.fi 100%[===================>] 413.02M  13.7MB/s    in 32s     \n",
            "\n",
            "2025-12-03 14:16:46 (13.1 MB/s) - ‘ubertext.fiction.filter_rus_gcld+short.text_only.txt.bz2’ saved [433084760/433084760]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS_FILE = \"ubertext.fiction.filter_rus_gcld+short.text_only.txt\"\n",
        "TOKENIZER_PATH = \"uk_bpe\" # Папка для токенізатора"
      ],
      "metadata": {
        "id": "G0q76QBNkRr8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train tokenizer"
      ],
      "metadata": {
        "id": "0VjQ909nd4Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Тренування токенізатора ---\")\n",
        "\n",
        "if not os.path.exists(os.path.join(TOKENIZER_PATH, \"vocab.json\")):\n",
        "    os.makedirs(TOKENIZER_PATH, exist_ok=True)\n",
        "\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "    print(\"Починаємо навчання BPE...\")\n",
        "    tokenizer.train(\n",
        "        files=[CORPUS_FILE],\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=5,\n",
        "        special_tokens=[\"<|endoftext|>\", \"<|pad|>\"]\n",
        "    )\n",
        "\n",
        "    vocab_size = tokenizer.get_vocab_size()\n",
        "    tokenizer.save_model(TOKENIZER_PATH)\n",
        "    print(f\"Токенізатор збережено в {TOKENIZER_PATH}, vocab_size = {vocab_size}\")\n",
        "else:\n",
        "    print(\"Токенізатор уже існує, пропускаємо навчання.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYTpVKfYklan",
        "outputId": "49ba0360-99a3-4425-d737-4fc79548ce1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Тренування токенізатора ---\n",
            "Починаємо навчання BPE...\n",
            "Токенізатор збережено в uk_bpe, vocab_size = 32768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Підготовка даних (Tokenization -> .bin)\n",
        "\n",
        "train_bin_path = \"train.bin\"\n",
        "val_bin_path = \"val.bin\"\n",
        "\n",
        "print(\"\\n--- Підготовка даних (Tokenization & Binaries) ---\")\n",
        "\n",
        "if not (os.path.exists(train_bin_path) and os.path.exists(val_bin_path)):\n",
        "\n",
        "    # Завантажуємо токенізатор\n",
        "    tokenizer = ByteLevelBPETokenizer(\n",
        "        os.path.join(TOKENIZER_PATH, \"vocab.json\"),\n",
        "        os.path.join(TOKENIZER_PATH, \"merges.txt\"),\n",
        "    )\n",
        "    tokenizer.add_special_tokens([\"<|endoftext|>\", \"<|pad|>\"])\n",
        "\n",
        "    print(\"Читання файлу та токенізація корпусу...\")\n",
        "\n",
        "    # Використовуємо uint16, бо vocab_size <= 65535\n",
        "    dtype = np.uint16\n",
        "    all_tokens = []\n",
        "\n",
        "    # Читаємо chunk-ами, щоб не з'їсти всю RAM\n",
        "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        while True:\n",
        "            chunk = f.read(1_000_000)  # 1M символів за раз\n",
        "            if not chunk:\n",
        "                break\n",
        "            ids = tokenizer.encode(chunk).ids\n",
        "            all_tokens.extend(ids)\n",
        "\n",
        "    print(f\"Всього токенів у корпусі: {len(all_tokens):,}\")\n",
        "\n",
        "    arr = np.array(all_tokens, dtype=dtype)\n",
        "\n",
        "    # train / val split 95/5\n",
        "    n = int(0.95 * len(arr))\n",
        "    train_data = arr[:n]\n",
        "    val_data = arr[n:]\n",
        "\n",
        "    print(f\"Train tokens: {len(train_data):,}\")\n",
        "    print(f\"Val tokens:   {len(val_data):,}\")\n",
        "\n",
        "    print(f\"Збереження {train_bin_path}...\")\n",
        "    train_data.tofile(train_bin_path)\n",
        "\n",
        "    print(f\"Збереження {val_bin_path}...\")\n",
        "    val_data.tofile(val_bin_path)\n",
        "\n",
        "    del arr, train_data, val_data\n",
        "else:\n",
        "    print(\"train.bin і val.bin вже існують, пропускаємо токенізацію.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smDKjwLOkn1n",
        "outputId": "bb1116c6-baeb-46b2-b31a-175a424d8dee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Підготовка даних (Tokenization & Binaries) ---\n",
            "Читання файлу та токенізація корпусу...\n",
            "Всього токенів у корпусі: 342,858,420\n",
            "Train tokens: 325,715,499\n",
            "Val tokens:   17,142,921\n",
            "Збереження train.bin...\n",
            "Збереження val.bin...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Завантаження даних ---\n",
        "\n",
        "# Завантажуємо токенізатор\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    os.path.join(TOKENIZER_PATH, \"vocab.json\"),\n",
        "    os.path.join(TOKENIZER_PATH, \"merges.txt\"),\n",
        ")\n",
        "tokenizer.add_special_tokens([\"<|endoftext|>\", \"<|pad|>\"])\n",
        "\n",
        "train_bin_path = \"train.bin\"\n",
        "val_bin_path = \"val.bin\"\n",
        "\n",
        "# np.memmap дозволяє працювати з файлом на диску як з масивом у пам'яті\n",
        "train_data = np.memmap(train_bin_path, dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(val_bin_path, dtype=np.uint16, mode='r')\n",
        "\n",
        "print(f\"Memmap Train size: {len(train_data):,} tokens\")\n",
        "print(f\"Memmap Val size: {len(val_data):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjBjIIgHk-Sm",
        "outputId": "b42de43c-ae97-4e93-a1c7-8aec48bfb909"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memmap Train size: 325,715,499 tokens\n",
            "Memmap Val size: 17,142,921 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Генеруємо випадкові індекси\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "    # Переміщення на GPU\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x = x.pin_memory().to(device, non_blocking=True)\n",
        "        y = y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "ZXKdB8J2mXkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "D8bHZHYEKk9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        # flash attention\n",
        "        y = torch.nn.functional.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.dropout if self.training else 0,\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # re-assemble all head outputs side by side\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # output projection with dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.sa = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # embedding table for each token\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # embedding table for positions\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # блоки трансформера\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head, block_size=block_size, dropout=dropout)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final normalization\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # output layer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets have shape (B, T)\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx це (B, T) масив індексів поточного контексту\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "EQ0_2cSMVm8v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "kfuyDGWDKikA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel().to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Кількість параметрів моделі: {num_params/1e6:.2f}M\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaSSz2qfFJqE",
        "outputId": "17c32933-14b7-42eb-c468-e1caf68b5fe9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кількість параметрів: 135.59M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Додаткові налаштування ---\n",
        "# Вибір типу даних для змішаної точності (економить пам'ять GPU)\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "\n",
        "# Ініціалізація GradScaler для коректної роботи з float16\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))\n",
        "\n",
        "# Параметри планувальника швидкості навчання\n",
        "warmup_iters = 1000 # Кількість кроків для \"розігріву\"\n",
        "min_lr = learning_rate / 10 # Мінімальний LR в кінці навчання\n",
        "grad_clip = 1.0 # Поріг для обрізання градієнтів\n",
        "\n",
        "# Функція динамічної зміни learning rate (Cosine Decay)\n",
        "def get_lr(it):\n",
        "    # Етап лінійного розігріву\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "\n",
        "    # Плавний спад за косинусоїдою\n",
        "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "ZcvOavOUOek_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "-r2F_6WDB3LO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Налаштування точності для TF32 (прискорює навчання)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "\n",
        "best_val_loss = 1e6\n",
        "\n",
        "print(f\"Починаємо навчання на пристрої: {device}\")\n",
        "\n",
        "for iter in tqdm(range(max_iters), desc=\"Навчання\"):\n",
        "\n",
        "    # 1. Оновлення швидкості навчання для поточної ітерації\n",
        "    lr = get_lr(iter)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # 2. Періодична оцінка моделі\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"\\nstep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.4e}\")\n",
        "\n",
        "        # Збереження найкращого чекпоінту\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "    # 3. Отримання наступного батча\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 4. Forward pass з використанням змішаної точності\n",
        "    with ctx:\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "    # 5. Backward pass з масштабуванням градієнтів\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # 6. Обрізання градієнтів для запобігання нестабільності\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # 7. Оновлення ваг та скейлера\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # 8. Очищення градієнтів\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "# Завантаження найкращої версії моделі після завершення\n",
        "model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
        "model.eval()\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ-XGTC-d5y7",
        "outputId": "cf3c2714-9fc5-4080-f1ba-60ad6c1b426c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model...\n",
            "Починаємо навчання на пристрої: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rНавчання:   0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1203 12:46:18.740000 13046 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 0: train loss 10.5603, val loss 10.5615, lr 5.9940e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  10%|█         | 1000/10000 [07:57<59:58,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 1000: train loss 5.7672, val loss 5.8521, lr 6.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  20%|██        | 2000/10000 [15:21<53:19,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 2000: train loss 5.0241, val loss 5.1333, lr 5.8372e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  30%|███       | 3000/10000 [22:45<46:36,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 3000: train loss 4.7173, val loss 4.8402, lr 5.3683e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  40%|████      | 4000/10000 [30:09<40:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 4000: train loss 4.5337, val loss 4.6690, lr 4.6500e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  50%|█████     | 5000/10000 [37:33<33:20,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 5000: train loss 4.3850, val loss 4.5299, lr 3.7689e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  60%|██████    | 6000/10000 [44:57<26:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 6000: train loss 4.2607, val loss 4.4285, lr 2.8311e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  70%|███████   | 7000/10000 [52:21<19:59,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 7000: train loss 4.1763, val loss 4.3520, lr 1.9500e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  80%|████████  | 8000/10000 [59:45<13:19,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 8000: train loss 4.1106, val loss 4.2918, lr 1.2317e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання:  90%|█████████ | 9000/10000 [1:07:09<06:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 9000: train loss 4.0607, val loss 4.2596, lr 7.6283e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання: 100%|█████████▉| 9999/10000 [1:14:33<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 9999: train loss 4.0413, val loss 4.2291, lr 6.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Навчання: 100%|██████████| 10000/10000 [1:15:18<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Генерація ---\n",
        "print(\"\\n--- Генерація тексту ---\\n\")\n",
        "\n",
        "start_text = \"\"\"Це основа всього\"\"\"\n",
        "start_ids = tokenizer.encode(start_text).ids\n",
        "context = torch.tensor([start_ids], dtype=torch.long, device=device)\n",
        "\n",
        "generated_ids = model.generate(context, max_new_tokens=200)[0].tolist()\n",
        "\n",
        "print(tokenizer.decode(generated_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeSB6-pmVj4S",
        "outputId": "be3621f4-1b96-4c62-9ccb-9c694f4d851e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Генерація тексту ---\n",
            "\n",
            "Це основа всього\n",
            "життя. Димиємо й сутнім, ми готові заглушити один про одного. Ти будеш\n",
            "порядний,— Там вирваними нас від ціле життя.\n",
            "— Прошу не чергувати! Я прошу вас! Я вас розведу на каменях,— вклонився\n",
            "Блеймасом і показав такому чоловікові, що зразу ж спинився.\n",
            "Блеймотом ми опинилися перед Марсельовою приготованою хатою і попрямували до\n",
            "нього.\n",
            "Тарас Іванович поволі, жадно й весело кружляючи по землі, виступав біля\n",
            "американців. Думки його поволі зближувалися з бастіонами божевільного\n",
            "Степа. Перед ними заволокла веселенька русалка, Маша підбігла до дівчини-відзиву,\n",
            " гукнула:\n",
            "— Прощайте!\n",
            "— Прощай.\n",
            "— Прощайте, прощайте.— І ми вийшли за місто. Я поцілувала його в руки,\n",
            "поцілувала, поцілувала, сказала:\n",
            "— Велике діло. — Він терпеливо Пальмачку\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "* https://github.com/karpathy/nanoGPT"
      ],
      "metadata": {
        "id": "2U-VGNjxDXHP"
      }
    }
  ]
}
