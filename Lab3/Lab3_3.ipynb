{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM8LR7uYiGO1UGraJSqaqT6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kostia9/Data-Analysis-2025/blob/main/Lab3/Lab3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import bz2\n",
        "import os\n",
        "import json\n",
        "\n",
        "# NLTK tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import userdata\n",
        "\n",
        "# Seeds and device\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "hiOKGJuagn7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4479d705-cd7d-4661-df4e-ab6dba4e9397"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7koGtUXubkF7",
        "outputId": "64ab5982-0c69-4633-9d69-73665f5e38d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
            "License(s): unknown\n",
            "Downloading amazonreviews.zip to /content\n",
            "100% 491M/493M [00:00<00:00, 1.72GB/s]\n",
            "100% 493M/493M [00:00<00:00, 1.71GB/s]\n",
            "Archive:  amazonreviews.zip\n",
            "  inflating: test.ft.txt.bz2         \n",
            "  inflating: train.ft.txt.bz2        \n"
          ]
        }
      ],
      "source": [
        "import os, json\n",
        "from google.colab import userdata\n",
        "\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "\n",
        "os.environ[\"KAGGLE_JSON\"] = userdata.get(\"KAGGLE_JSON\")\n",
        "kaggle_json = os.environ['KAGGLE_JSON']\n",
        "kaggle_token = json.loads(kaggle_json)\n",
        "\n",
        "# Записати у файл kaggle.json\n",
        "kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "with open(kaggle_path, \"w\") as f:\n",
        "    json.dump(kaggle_token, f)\n",
        "os.chmod(kaggle_path, 0o600)\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "!kaggle datasets download -d bittlingmayer/amazonreviews\n",
        "!unzip -n amazonreviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Load, clean, tokenize ---\n",
        "def extractData(filename, n_samples=None):\n",
        "    data = []\n",
        "    with bz2.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_samples is not None and i >= n_samples:\n",
        "                break\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) == 2:\n",
        "                label, text = parts\n",
        "                data.append((label, text))\n",
        "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
        "    df['label'] = df['label'].str.extract(r'(\\d+)').astype(int)\n",
        "    # Keep only labels {1,2} then map to {0,1}\n",
        "    df = df[df['label'].isin([1, 2])].reset_index(drop=True)\n",
        "    df['label'] = df['label'] - 1\n",
        "    return df\n",
        "\n",
        "# Smaller sample for speed; increase if you want better accuracy\n",
        "train_df = extractData('train.ft.txt.bz2', n_samples=500_000)\n",
        "test_df  = extractData('test.ft.txt.bz2',  n_samples=50_000)\n",
        "\n",
        "print(f\"Loaded {len(train_df)} train and {len(test_df)} test samples.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "print(\"Cleaning...\")\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['clean_text']  = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "train_df['tokens'] = train_df['clean_text'].apply(word_tokenize)\n",
        "test_df['tokens']  = test_df['clean_text'].apply(word_tokenize)\n",
        "\n",
        "print(train_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXrLMfoYbww-",
        "outputId": "99649905-be28-45ec-9021-385eb84fad3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500000 train and 50000 test samples.\n",
            "Cleaning...\n",
            "Tokenizing...\n",
            "   label                                               text  \\\n",
            "0      1  Stuning even for the non-gamer: This sound tra...   \n",
            "1      1  The best soundtrack ever to anything.: I'm rea...   \n",
            "2      1  Amazing!: This soundtrack is my favorite music...   \n",
            "\n",
            "                                          clean_text  \\\n",
            "0  stuning even for the non gamer this sound trac...   \n",
            "1  the best soundtrack ever to anything i m readi...   \n",
            "2  amazing this soundtrack is my favorite music o...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [stuning, even, for, the, non, gamer, this, so...  \n",
            "1  [the, best, soundtrack, ever, to, anything, i,...  \n",
            "2  [amazing, this, soundtrack, is, my, favorite, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Build vocab ---\n",
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for tok_list in train_df['tokens']:\n",
        "    counter.update(tok_list)\n",
        "\n",
        "MAX_VOCAB = 30_000\n",
        "specials = [\"[PAD]\", \"[UNK]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]\n",
        "stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "vocab_size = len(stoi)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM54mFpFbxg7",
        "outputId": "e17df08f-f5ad-4a8e-c195-8b157e968044"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Encode and pad ---\n",
        "def encode(tokens):\n",
        "    return [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "MAX_LEN = 200\n",
        "def pad_and_encode(tokens):\n",
        "    seq = encode(tokens)\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "print(\"Encoding...\")\n",
        "X_train_full = torch.stack([pad_and_encode(seq) for seq in train_df['tokens']])\n",
        "y_train_full = torch.tensor(train_df['label'].values, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.stack([pad_and_encode(seq) for seq in test_df['tokens']])\n",
        "y_test = torch.tensor(test_df['label'].values, dtype=torch.float32)\n",
        "\n",
        "print(\"X_train_full:\", X_train_full.shape)\n",
        "print(\"y_train_full:\", y_train_full.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879olpAqcx06",
        "outputId": "8a7ffde3-e640-43bf-f591-30837f510dce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding...\n",
            "X_train_full: torch.Size([500000, 100])\n",
            "y_train_full: torch.Size([500000])\n",
            "X_test: torch.Size([50000, 100])\n",
            "y_test: torch.Size([50000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Datasets and loaders ---\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "full_train_dataset = TensorDataset(X_train_full, y_train_full)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(full_train_dataset) * VAL_FRAC)\n",
        "train_sz = len(full_train_dataset) - val_sz\n",
        "train_ds, val_ds = random_split(full_train_dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOpgSHzC7US",
        "outputId": "6c8decce-a920-4989-b730-52043d952a29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 400000 | Val: 100000 | Test: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Model ---\n",
        "class GRUNet(nn.Module):\n",
        "    # Додаємо pad_idx\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, pad_idx, n_layers, drop_prob=0.5):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Використовуємо pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                          batch_first=True,\n",
        "                          dropout=drop_prob,\n",
        "                          bidirectional=False) # Однонаправлена GRU\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x) # [B, T, E]\n",
        "\n",
        "        # gru_out: [B, T, H]\n",
        "        # hidden: [n_layers, B, H]\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "\n",
        "        # Використовуємо останній прихований стан для класифікації\n",
        "        # hidden[-1] - це прихований стан останнього шару\n",
        "        last_hidden_state = hidden[-1, :, :] # Shape: [B, H]\n",
        "\n",
        "        out = self.dropout(last_hidden_state)\n",
        "        out = self.fc(out)    # [B, 1]\n",
        "        out = self.sigmoid(out) # [B, 1]\n",
        "\n",
        "        if self.output_size == 1:\n",
        "            out = out.squeeze(1) # [B]\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "_1_4zQ0yRPaa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        h = model.init_hidden(x.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out, h = model(x, h)\n",
        "\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(loader)\n",
        "    return train_loss\n",
        "\n",
        "def evaluate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for x, y in loader:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "\n",
        "          h = model.init_hidden(x.size(0))\n",
        "          out, h = model(x, h)\n",
        "\n",
        "          loss = criterion(out, y)\n",
        "          valid_loss += loss.item()\n",
        "\n",
        "    valid_loss /= len(loader)\n",
        "    return valid_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        h = model.init_hidden(xb.size(0))\n",
        "        logits, h = model(xb, h)\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        # Отримуємо прогнози (0 або 1)\n",
        "        preds = (logits >= 0.5).long()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        total_acc  += (preds == yb.long()).sum().item()\n",
        "        n += xb.size(0)\n",
        "\n",
        "    return total_loss/n, total_acc/n"
      ],
      "metadata": {
        "id": "otmHBBAzRQoV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Train baseline model (random init embeddings) ---\n",
        "emb_dim = 100\n",
        "hidden_dim = 128\n",
        "pad_idx = PAD_IDX\n",
        "\n",
        "# Нові параметри для GRUNet\n",
        "output_size = 1\n",
        "n_layers = 2\n",
        "drop_prob = 0.5\n",
        "\n",
        "model_basic = GRUNet(\n",
        "    vocab_size, output_size, emb_dim, hidden_dim, pad_idx, n_layers, drop_prob\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_basic.parameters(), lr=1e-3)\n",
        "\n",
        "n_epochs = 5\n",
        "print(model_basic)\n",
        "\n",
        "print(\"\\n--- Training: baseline ---\")\n",
        "for epoch in range(n_epochs):\n",
        "    tr = train_one_epoch(model_basic, train_loader, optimizer, criterion, device)\n",
        "    va = evaluate_one_epoch(model_basic, val_loader, criterion, device)\n",
        "    print(f\"[{epoch+1:02d}] train_loss={tr:.4f} | val_loss={va:.4f}\")\n",
        "\n",
        "test_loss, test_acc = evaluate(model_basic, test_loader, criterion, device)\n",
        "print(f\"\\n[Baseline] TEST loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX1Za2KcRU7F",
        "outputId": "107af869-1f98-4f0c-a075-5141b3e44d6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUNet(\n",
            "  (embedding): Embedding(20000, 100, padding_idx=0)\n",
            "  (gru): GRU(100, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "--- Training: baseline ---\n",
            "[01] train_loss=0.2949 | val_loss=0.1951\n",
            "[02] train_loss=0.1770 | val_loss=0.1790\n",
            "[03] train_loss=0.1494 | val_loss=0.1753\n",
            "[04] train_loss=0.1275 | val_loss=0.1799\n",
            "[05] train_loss=0.1060 | val_loss=0.1909\n",
            "\n",
            "[Baseline] TEST loss=0.1966  acc=93.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Завантаження fastText ---\")\n",
        "if not os.path.exists(\"wiki-news-300d-1M.vec.zip\"):\n",
        "    print(\"Downloading fastText vectors (1.6GB)...\")\n",
        "    !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "print(\"Unzipping vectors...\")\n",
        "!unzip -q -n wiki-news-300d-1M.vec.zip\n",
        "\n",
        "def load_fasttext_vec(path):\n",
        "    vectors = {}\n",
        "    dim = 0\n",
        "    print(\"Loading fastText vectors into memory...\")\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        first_line_parts = f.readline().rstrip().split(\" \")\n",
        "        num_words = int(first_line_parts[0])\n",
        "        dim = int(first_line_parts[1])\n",
        "\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                if len(parts) == dim + 1:\n",
        "                    vec = np.asarray(parts[1:], dtype=np.float32)\n",
        "                    vectors[word] = vec\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return vectors, dim\n",
        "\n",
        "ft_path = \"wiki-news-300d-1M.vec\"\n",
        "fasttext_vectors, emb_dim_ft = load_fasttext_vec(ft_path)\n",
        "print(f\"fastText dim: {emb_dim_ft} | loaded entries: {len(fasttext_vectors):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBr4rxzVR4g5",
        "outputId": "0d8f06b3-2c17-43e5-e70f-2603953e0be8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Завантаження fastText ---\n",
            "Downloading fastText vectors (1.6GB)...\n",
            "Unzipping vectors...\n",
            "Loading fastText vectors into memory...\n",
            "fastText dim: 300 | loaded entries: 999,994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Створення матриці ембедингів...\")\n",
        "emb_dim = emb_dim_ft # Тепер 300\n",
        "\n",
        "emb_matrix = np.random.normal(scale=0.1, size=(vocab_size, emb_dim)).astype(np.float32)\n",
        "emb_matrix[PAD_IDX] = 0.0\n",
        "\n",
        "hit = 0\n",
        "for w, idx in stoi.items():\n",
        "    v = fasttext_vectors.get(w)\n",
        "    if v is not None:\n",
        "        emb_matrix[idx] = v\n",
        "        hit += 1\n",
        "print(f\"Coverage: {hit}/{vocab_size} = {hit/vocab_size:.1%}\")\n",
        "\n",
        "pretrained_emb = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(emb_matrix),\n",
        "    freeze=True,\n",
        "    padding_idx=PAD_IDX\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pDSayBHSLJH",
        "outputId": "3be0133a-9f01-485b-8bb3-3894ce9dc113"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Створення матриці ембедингів...\n",
            "Coverage: 19380/20000 = 96.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 11. Train model ---\n",
        "model_tl = GRUNet(\n",
        "    vocab_size, output_size, emb_dim, hidden_dim, pad_idx, n_layers, drop_prob\n",
        ").to(device)\n",
        "\n",
        "model_tl.embedding = pretrained_emb\n",
        "model_tl.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_tl.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"\\n--- training fastText (GRU) ---\")\n",
        "for epoch in range(n_epochs):\n",
        "    tr = train_one_epoch(model_tl, train_loader, optimizer, criterion, device)\n",
        "    va = evaluate_one_epoch(model_tl, val_loader, criterion, device)\n",
        "    print(f\"[{epoch+1:02d}] train_loss={tr:.4f} | val_loss={va:.4f}\")\n",
        "\n",
        "test_loss, test_acc = evaluate(model_tl, test_loader, criterion, device)\n",
        "print(f\"\\n[fastText GRU Model] TEST  loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmX5O1h3SObN",
        "outputId": "594c2c69-564e-44a4-b172-eeda764a2680"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- training fastText (GRU) ---\n",
            "[01] train_loss=0.2914 | val_loss=0.2019\n",
            "[02] train_loss=0.1897 | val_loss=0.1773\n",
            "[03] train_loss=0.1706 | val_loss=0.1678\n",
            "[04] train_loss=0.1582 | val_loss=0.1661\n",
            "[05] train_loss=0.1479 | val_loss=0.1612\n",
            "\n",
            "[fastText GRU Model] TEST  loss=0.1679  acc=93.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPjHXzWvS9UX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}