{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kostia9/Data-Analysis-2025/blob/main/Lab3/Lab3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiOKGJuagn7_",
        "outputId": "0564432c-a69b-4923-9955-6d60617411bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import bz2\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "\n",
        "# NLTK tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import userdata\n",
        "\n",
        "# Seeds and device\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7koGtUXubkF7",
        "outputId": "a45ee29c-640a-4f3e-9496-cf79c1f778be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
            "License(s): unknown\n",
            "Downloading amazonreviews.zip to /content\n",
            " 67% 329M/493M [00:00<00:00, 1.73GB/s]\n",
            "100% 493M/493M [00:00<00:00, 1.73GB/s]\n",
            "Archive:  amazonreviews.zip\n",
            "  inflating: test.ft.txt.bz2         \n",
            "  inflating: train.ft.txt.bz2        \n"
          ]
        }
      ],
      "source": [
        "import os, json\n",
        "from google.colab import userdata\n",
        "\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "\n",
        "os.environ[\"KAGGLE_JSON\"] = userdata.get(\"KAGGLE_JSON\")\n",
        "kaggle_json = os.environ['KAGGLE_JSON']\n",
        "kaggle_token = json.loads(kaggle_json)\n",
        "\n",
        "kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "with open(kaggle_path, \"w\") as f:\n",
        "    json.dump(kaggle_token, f)\n",
        "os.chmod(kaggle_path, 0o600)\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "!kaggle datasets download -d bittlingmayer/amazonreviews\n",
        "!unzip -n amazonreviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXrLMfoYbww-",
        "outputId": "a4b81c66-6fb9-4c94-b14a-e94b9a48e8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 200000 train and 400000 test samples.\n",
            "Cleaning...\n",
            "Tokenizing...\n",
            "   label                                               text  \\\n",
            "0      1  Stuning even for the non-gamer: This sound tra...   \n",
            "1      1  The best soundtrack ever to anything.: I'm rea...   \n",
            "2      1  Amazing!: This soundtrack is my favorite music...   \n",
            "\n",
            "                                          clean_text  \\\n",
            "0  stuning even for the non gamer this sound trac...   \n",
            "1  the best soundtrack ever to anything i m readi...   \n",
            "2  amazing this soundtrack is my favorite music o...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [stuning, even, for, the, non, gamer, this, so...  \n",
            "1  [the, best, soundtrack, ever, to, anything, i,...  \n",
            "2  [amazing, this, soundtrack, is, my, favorite, ...  \n"
          ]
        }
      ],
      "source": [
        "# --- 2. Load, clean, tokenize ---\n",
        "def extractData(filename, n_samples=None):\n",
        "    data = []\n",
        "    with bz2.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_samples is not None and i >= n_samples:\n",
        "                break\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) == 2:\n",
        "                label, text = parts\n",
        "                data.append((label, text))\n",
        "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
        "    df['label'] = df['label'].str.extract(r'(\\d+)').astype(int)\n",
        "    # Keep only labels {1,2} then map to {0,1}\n",
        "    df = df[df['label'].isin([1, 2])].reset_index(drop=True)\n",
        "    df['label'] = df['label'] - 1\n",
        "    return df\n",
        "\n",
        "train_df = extractData('train.ft.txt.bz2', n_samples=200_000)\n",
        "test_df  = extractData('test.ft.txt.bz2')\n",
        "\n",
        "print(f\"Loaded {len(train_df)} train and {len(test_df)} test samples.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "print(\"Cleaning...\")\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['clean_text']  = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "train_df['tokens'] = train_df['clean_text'].apply(word_tokenize)\n",
        "test_df['tokens']  = test_df['clean_text'].apply(word_tokenize)\n",
        "\n",
        "print(train_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM54mFpFbxg7",
        "outputId": "60a2dd1d-bc00-41cf-8e9f-d1ba76f9f9a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 20000\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Build vocab ---\n",
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for tok_list in train_df['tokens']:\n",
        "    counter.update(tok_list)\n",
        "\n",
        "MAX_VOCAB = 20_000\n",
        "specials = [\"[PAD]\", \"[UNK]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]\n",
        "stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "vocab_size = len(stoi)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVxbXalV9DgV",
        "outputId": "c83c7a16-b6c3-4952-b952-b437333d30f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Length Statistics ===\n",
            "Count: 200000\n",
            "Min: 3\n",
            "Max: 242\n",
            "Mean: 81.982425\n",
            "Median: 74.0\n",
            "90th percentile: 150.0\n",
            "95th percentile: 167.0\n",
            "99th percentile: 186.0\n"
          ]
        }
      ],
      "source": [
        "lengths = np.array([len(toks) for toks in train_df['tokens']])\n",
        "\n",
        "print(\"=== Length Statistics ===\")\n",
        "print(\"Count:\", len(lengths))\n",
        "print(\"Min:\", lengths.min())\n",
        "print(\"Max:\", lengths.max())\n",
        "print(\"Mean:\", lengths.mean())\n",
        "print(\"Median:\", np.median(lengths))\n",
        "print(\"90th percentile:\", np.percentile(lengths, 90))\n",
        "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
        "print(\"99th percentile:\", np.percentile(lengths, 99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879olpAqcx06",
        "outputId": "6a1eb51e-9b47-4622-f37b-84cf0b4ee7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding...\n",
            "X_train_full: torch.Size([200000, 170])\n",
            "y_train_full: torch.Size([200000])\n",
            "X_test: torch.Size([400000, 170])\n",
            "y_test: torch.Size([400000])\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN = 170\n",
        "\n",
        "# --- 4. Encode and pad ---\n",
        "def encode(tokens):\n",
        "    return [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "def pad_and_encode(tokens):\n",
        "    seq = encode(tokens)\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "print(\"Encoding...\")\n",
        "X_train_full = torch.stack([pad_and_encode(seq) for seq in train_df['tokens']])\n",
        "y_train_full = torch.tensor(train_df['label'].values, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.stack([pad_and_encode(seq) for seq in test_df['tokens']])\n",
        "y_test = torch.tensor(test_df['label'].values, dtype=torch.float32)\n",
        "\n",
        "print(\"X_train_full:\", X_train_full.shape)\n",
        "print(\"y_train_full:\", y_train_full.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOpgSHzC7US",
        "outputId": "e29a3a68-c1e9-4220-975a-a7c42b673d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 160000 | Val: 40000 | Test: 400000\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Datasets and loaders ---\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "full_train_dataset = TensorDataset(X_train_full, y_train_full)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(full_train_dataset) * VAL_FRAC)\n",
        "train_sz = len(full_train_dataset) - val_sz\n",
        "train_ds, val_ds = random_split(full_train_dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_1_4zQ0yRPaa"
      },
      "outputs": [],
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, n_layers, drop_prob=0.5):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                          batch_first=True,\n",
        "                          dropout=drop_prob if n_layers > 1 else 0)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x.long())\n",
        "\n",
        "        gru_out, _ = self.gru(embeds)\n",
        "        pooled = torch.max(gru_out, dim=1)[0]\n",
        "\n",
        "        out = self.dropout(pooled)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "otmHBBAzRQoV"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(x)\n",
        "\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss / len(loader)\n",
        "\n",
        "def evaluate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for x, y in loader:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "\n",
        "          out = model(x)\n",
        "\n",
        "          loss = criterion(out, y)\n",
        "          valid_loss += loss.item()\n",
        "\n",
        "    valid_loss /= len(loader)\n",
        "    return valid_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        total_acc  += (preds == yb.long()).sum().item()\n",
        "        n += xb.size(0)\n",
        "\n",
        "    return total_loss/n, total_acc/n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX1Za2KcRU7F",
        "outputId": "3403cc7a-ba7b-40f9-ce96-d006871f59e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training: baseline ---\n",
            "[01] train_loss=0.3289 | val_loss=0.2390\n",
            "[02] train_loss=0.2265 | val_loss=0.2150\n",
            "[03] train_loss=0.2024 | val_loss=0.2099\n",
            "[04] train_loss=0.1857 | val_loss=0.1886\n",
            "[05] train_loss=0.1740 | val_loss=0.1828\n",
            "[06] train_loss=0.1657 | val_loss=0.1853\n",
            "[07] train_loss=0.1572 | val_loss=0.1760\n",
            "[08] train_loss=0.1495 | val_loss=0.1744\n",
            "[09] train_loss=0.1427 | val_loss=0.1766\n",
            "[10] train_loss=0.1364 | val_loss=0.1770\n",
            "[11] train_loss=0.1286 | val_loss=0.1786\n",
            "[12] train_loss=0.1226 | val_loss=0.1820\n",
            "[13] train_loss=0.1163 | val_loss=0.1808\n",
            "Early stopping at epoch 12 (no improvement for 5 epochs).\n",
            "\n",
            "[Baseline] TEST loss=0.1733  acc=93.33%\n"
          ]
        }
      ],
      "source": [
        "# --- 8. Train baseline model (random init embeddings) ---\n",
        "emb_dim = 300\n",
        "hidden_dim = 128\n",
        "pad_idx = PAD_IDX\n",
        "\n",
        "n_layers = 2\n",
        "\n",
        "model_basic = GRUNet(vocab_size, emb_dim, hidden_dim, pad_idx, n_layers).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_basic.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "n_epochs = 20\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "best_val_loss = 1e5\n",
        "\n",
        "print(\"\\n--- Training: baseline ---\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train_one_epoch(model_basic, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate_one_epoch(model_basic, val_loader, criterion, device)\n",
        "    print(f\"[{epoch+1:02d}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "    # improvement?\n",
        "    if val_loss < best_val_loss - 1e-6:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = copy.deepcopy(model_basic.state_dict())\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
        "            break\n",
        "\n",
        "# load best weights\n",
        "model_basic.load_state_dict(best_state)\n",
        "\n",
        "test_loss, test_acc = evaluate(model_basic, test_loader, criterion, device)\n",
        "print(f\"\\n[Baseline] TEST loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Завантаження fastText ---\")\n",
        "if not os.path.exists(\"wiki-news-300d-1M.vec.zip\"):\n",
        "    print(\"Downloading fastText vectors (1.6GB)...\")\n",
        "    !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "print(\"Unzipping vectors...\")\n",
        "!unzip -q -n wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "id": "lBr4rxzVR4g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ca092b-fb59-4060-967b-e25732bb0904"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Завантаження fastText ---\n",
            "Downloading fastText vectors (1.6GB)...\n",
            "Unzipping vectors...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fasttext_vec(path):\n",
        "    vectors = {}\n",
        "    dim = 0\n",
        "    print(\"Loading fastText vectors into memory...\")\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        first_line_parts = f.readline().rstrip().split(\" \")\n",
        "        num_words = int(first_line_parts[0])\n",
        "        dim = int(first_line_parts[1])\n",
        "\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                if len(parts) == dim + 1:\n",
        "                    vec = np.asarray(parts[1:], dtype=np.float32)\n",
        "                    vectors[word] = vec\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return vectors, dim\n",
        "\n",
        "ft_path = \"wiki-news-300d-1M.vec\"\n",
        "fasttext_vectors, emb_dim_ft = load_fasttext_vec(ft_path)\n",
        "print(f\"fastText dim: {emb_dim_ft} | loaded entries: {len(fasttext_vectors):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Rb3krj7VNz",
        "outputId": "d0f3c7e8-243b-4b94-a11c-56fdd623c48e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fastText vectors into memory...\n",
            "fastText dim: 300 | loaded entries: 999,994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6pDSayBHSLJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6caac0-9539-4c12-ad83-eca9472e5b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Створення матриці ембедингів...\n",
            "Coverage: 19300/20000 = 96.5%\n"
          ]
        }
      ],
      "source": [
        "print(\"Створення матриці ембедингів...\")\n",
        "emb_dim = emb_dim_ft\n",
        "\n",
        "emb_matrix = np.random.normal(scale=0.1, size=(vocab_size, emb_dim)).astype(np.float32)\n",
        "emb_matrix[PAD_IDX] = 0.0\n",
        "\n",
        "hit = 0\n",
        "for w, idx in stoi.items():\n",
        "    v = fasttext_vectors.get(w)\n",
        "    if v is not None:\n",
        "        emb_matrix[idx] = v\n",
        "        hit += 1\n",
        "print(f\"Coverage: {hit}/{vocab_size} = {hit/vocab_size:.1%}\")\n",
        "\n",
        "pretrained_emb = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(emb_matrix),\n",
        "    freeze=True,\n",
        "    padding_idx=PAD_IDX\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmX5O1h3SObN",
        "outputId": "a59b75bb-513e-47b0-f9f5-be407e7d659c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- training fastText (GRU) ---\n",
            "[01] train_loss=0.3226 | val_loss=0.2451\n",
            "[02] train_loss=0.2385 | val_loss=0.2306\n",
            "[03] train_loss=0.2202 | val_loss=0.2121\n",
            "[04] train_loss=0.2092 | val_loss=0.2089\n",
            "[05] train_loss=0.1987 | val_loss=0.2063\n",
            "[06] train_loss=0.1940 | val_loss=0.1986\n",
            "[07] train_loss=0.1867 | val_loss=0.1870\n",
            "[08] train_loss=0.1814 | val_loss=0.1847\n",
            "[09] train_loss=0.1769 | val_loss=0.1825\n",
            "[10] train_loss=0.1738 | val_loss=0.1802\n",
            "[11] train_loss=0.1689 | val_loss=0.1877\n",
            "[12] train_loss=0.1667 | val_loss=0.1751\n",
            "[13] train_loss=0.1627 | val_loss=0.1794\n",
            "[14] train_loss=0.1612 | val_loss=0.1762\n",
            "[15] train_loss=0.1579 | val_loss=0.1784\n",
            "Early stopping at epoch 14 (no improvement for 3 epochs).\n",
            "\n",
            "[fastText GRU Model] TEST  loss=0.1740  acc=93.27%\n"
          ]
        }
      ],
      "source": [
        "# --- 11. Train model ---\n",
        "model_tl = GRUNet(vocab_size, emb_dim, hidden_dim, pad_idx, n_layers).to(device)\n",
        "\n",
        "model_tl.embedding = pretrained_emb\n",
        "model_tl.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_tl.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "print(\"\\n--- training fastText (GRU) ---\")\n",
        "\n",
        "best_val_loss = 1e5\n",
        "patience_counter = 0\n",
        "patience = 3\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train_one_epoch(model_tl, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate_one_epoch(model_tl, val_loader, criterion, device)\n",
        "    print(f\"[{epoch+1:02d}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "    # improvement?\n",
        "    if val_loss < best_val_loss - 1e-6:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = copy.deepcopy(model_tl.state_dict())\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
        "            break\n",
        "\n",
        "model_tl.load_state_dict(best_state)\n",
        "test_loss, test_acc = evaluate(model_tl, test_loader, criterion, device)\n",
        "print(f\"\\n[fastText GRU Model] TEST  loss={test_loss:.4f}  acc={test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXnp7Er87ZdC"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyOirnsKXx1bbNTt8FD2P56H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}